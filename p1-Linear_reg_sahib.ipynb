{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "temporal-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regresssion equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "revised-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-announcement",
   "metadata": {},
   "source": [
    "### Creating inputs for Linear regression equations\n",
    "\n",
    "**torch.randn(size)** \n",
    "returns samples from the \"standard normal\" distribution [Ïƒ = 1]\n",
    "\n",
    "Unlike rand which is uniform, values closer to zero are more likely to appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "wireless-georgia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(46)\n",
    "\n",
    "\n",
    "x1= torch.randint(0,20,(50,2),dtype=torch.float)\n",
    "print(x1.shape)\n",
    "\n",
    "\n",
    "# x2= torch.randint(0,30,(50,1),dtype=torch.float)\n",
    "# print(x1.shape)\n",
    "\n",
    "# bias term is b\n",
    "b = torch.randint(1,10,(1,50)).reshape(-1,1)\n",
    "\n",
    "\n",
    "# equation will be  y = 2*x1[0] + 4*x1[1] + b\n",
    "y = 2*x1[0] + 4*x1[1]  +b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "functioning-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cannot be used while plotting MULTI VARIABLE LINEAR REGRESSION model\n",
    "# plt.scatter(x1.numpy(),y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-following",
   "metadata": {},
   "source": [
    "### Simple Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "whole-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linearmodel(nn.Module):\n",
    "    def __init__(self,input_f,output_f):\n",
    "        \n",
    "        # inherating every fucntion and Method present in nn.Module\n",
    "        super().__init__()\n",
    "        \n",
    "        # defining linear layer using Linear Model\n",
    "        self.linear_layer = nn.Linear(input_f,output_f)\n",
    "        \n",
    "        \n",
    "    def forward(self,x1):\n",
    "        prediction = self.linear_layer(x1)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "legitimate-defense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linearmodel(\n",
      "  (linear_layer): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "weight are  weight 1 is ---> 0.6408010721206665 weight 2 is ---> 0.08583964407444\n",
      "Bias is  -0.20712655782699585\n",
      "linear_layer.weight\n",
      "linear_layer.bias\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(56)\n",
    "\n",
    "model = Linearmodel(2,1)\n",
    "\n",
    "print(model)\n",
    "\n",
    "print(\"weight are  weight 1 is --->\",model.linear_layer.weight[0][0].item(),\"weight 2 is --->\",model.linear_layer.weight[0][1].item())\n",
    "print(\"Bias is \", model.linear_layer.bias.item())\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "retired-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # using one random input \n",
    "# x1_ = torch.tensor([[2.0,]]).T\n",
    "# print(x1_.shape)\n",
    "# # model is giving answer using y = wx+b  where w = 0.906 and b = 0.121\n",
    "# print(model.forward(x1_))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-expansion",
   "metadata": {},
   "source": [
    "### Now Taking LOSS function into ACCOUNT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-trick",
   "metadata": {},
   "source": [
    "## Set the loss function\n",
    "We could write our own function to apply a Mean Squared Error (MSE) that follows<br>\n",
    "\n",
    "$\\begin{split}MSE &= \\frac {1} {n} \\sum_{i=1}^n {(y_i - \\hat y_i)}^2 \\\\\n",
    "&= \\frac {1} {n} \\sum_{i=1}^n {(y_i - (wx_i + b))}^2\\end{split}$<br>\n",
    "\n",
    "Fortunately PyTorch has it built in.<br>\n",
    "<em>By convention, you'll see the variable name \"criterion\" used, but feel free to use something like \"linear_loss_func\" if that's clearer.</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-compatibility",
   "metadata": {},
   "source": [
    "## Set the optimization\n",
    "Here we'll use <a href='https://en.wikipedia.org/wiki/Stochastic_gradient_descent'>Stochastic Gradient Descent</a> (SGD) with an applied <a href='https://en.wikipedia.org/wiki/Learning_rate'>learning rate</a> (lr) of 0.001. Recall that the learning rate tells the optimizer how much to adjust each parameter on the next round of calculations. Too large a step and we run the risk of overshooting the minimum, causing the algorithm to diverge. Too small and it will take a long time to converge.\n",
    "\n",
    "For more complicated (multivariate) data, you might also consider passing optional <a href='https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum'><tt>momentum</tt></a> and <a href='https://en.wikipedia.org/wiki/Tikhonov_regularization'><tt>weight_decay</tt></a> arguments. Momentum allows the algorithm to \"roll over\" small bumps to avoid local minima that can cause convergence too soon. Weight decay (also called an L2 penalty) applies to biases.\n",
    "\n",
    "For more information, see <a href='https://pytorch.org/docs/stable/optim.html'><strong><tt>torch.optim</tt></strong></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "established-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer_ = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "experienced-montgomery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1000  loss: 3483.73803711\n",
      "epoch: 2000  loss: 3287.62963867\n",
      "epoch: 3000  loss: 3100.74926758\n",
      "epoch: 4000  loss: 2922.60131836\n",
      "epoch: 5000  loss: 2752.91430664\n",
      "epoch: 6000  loss: 2591.53588867\n",
      "epoch: 7000  loss: 2438.39916992\n",
      "epoch: 8000  loss: 2293.44287109\n",
      "epoch: 9000  loss: 2156.63696289\n",
      "epoch: 10000  loss: 2027.94189453\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    i+=1\n",
    "   \n",
    "   \n",
    "    y_pred = model.forward(x1)\n",
    "    loss = criterion(y_pred, y)\n",
    "    losses.append(loss)\n",
    "    if i%1000==0:\n",
    "        print(f'epoch: {i:2}  loss: {loss.item():10.8f}') \n",
    "    optimizer_.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "inside-vision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6941065788269043 1.142569899559021 0.8597599864006042\n"
     ]
    }
   ],
   "source": [
    "w1,w2,b1 = model.linear_layer.weight[0][0].item(), model.linear_layer.weight[0][1].item(),model.linear_layer.bias.item()\n",
    "print(w1,w2,b1)\n",
    "# y1 = x1*w1 +b1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-hardwood",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bita03373cad2404f55bdc5db0285b9fbe0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
